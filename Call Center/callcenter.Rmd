---
title: "Call Center Data"
author: "Syed Rahman"
date: "2/6/2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(pracma)
library(forecast)
library(chron)
library(dplyr)
library(xts)
library(reshape2)
library(ggplot2)
library(lassoshooting)
library(caret)
source('holidays.r')
source('CSCS2.r')
```

## Call Center Data Analysis

The goal is to predict the number of calls coming in during the second half of the day using the number of calls recieved during the first half of the day. To this end suppose $x_1$ and $x_2$ is the number of calls received during the first half of the day and the second half of the day, respectively. Let $x = (x_1^t, x_2^t)^t$ and $y = \sqrt{x + \frac{1}{4}}$. It can be shown that if $x \sim Poisson(\lambda)$, then $y \overset{.}{\sim} \mathcal{N}(\mu,\Sigma)$. In such a case, $y_2|y_1 \sim \mathcal{N}(\mu_2 + \Sigma_{21} \Sigma_{11}^{-1} (y_1 - \mu_1), \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12})$. Hence the best mean squared predictor of $y_2$ given $y_1$ (i.e. the preidctor that minimizes mean squared error) is 
$$\mathbb{E}[y_2|y_1] = \mu_2 + \Sigma_{21} \Sigma_{11}^{-1} (y_1 - \mu_1).$$ 
This is done using the follow code:
```{r predict, warning=FALSE}
predict.mean <- function(x1,mu,Sigma){
    p1 <- length(x1)
    p <- length(mu)
    p2 <- p-p1
    mu1 <- mu[1:p1]
    mu2 <- mu[(p1+1):p]
    Sigma11 <- Sigma[1:p1,1:p1]
    #Sigma12 <- Sigma[1:p1,(p1+1):p]
    Sigma21 <- Sigma[(p1+1):p,1:p1]
    #Sigma22 <- Sigma[(p1+1):p,(p1+1):p]
    x2 <- mu2 + Sigma21%*%solve(Sigma11,x1-mu1)
    return(x2)
}
```

Hence we must estimate $\mu_2$ and $\Sigma$ from the data. To this end, we divide the dataset into a training datset and a test dataset. Typically we do this in a random fashion, but since this is time-series data, I take the first 70% of the data to be the training dataset and the remaining 30% of the dataset to be the test dataset. Once we have the estimates, we make predictions for the calls coming in for the second half of the day using the calls that came in for the first half of the day for the test data set and then compare with true values.

The data was obtained from http://iew3.technion.ac.il/serveng/callcenterdata, which contains daily data for a call center for an Israeli bank for the year 1999. Four days are missing. But for all the other 361 days we have information for time of call, length of call, etc. As the call center was only open from 7AM to midnight, I removed all the data points outside of this interval. 

```{r data, warning=FALSE}
months = format(ISOdate(1999,1:12,1),"%B")
months = tolower(months)
data = read.table("january.txt",header=TRUE)
for(month in months[-1]){
    data = rbind(data,read.table(paste(month,".txt",sep=""),header=TRUE))
}
head(data,n = 6)
```

I divide the day up into 10 minute chunks and the first goal is to reformat the data so that there is an easy way to view the number of calls received at the call center on each day for each time period.

```{r dataclean, warning=FALSE}
data = filter(data, outcome=="AGENT")
data = select(data,date,vru_entry)
data$timedate = paste(data$date,data$vru_entry)
data$timedate = strptime(data$timedate,format='%y%m%d %H:%M:%S')
tt = seq(from = ISOdate(1999,1,1,0,0,0,tz = "EST"), to = ISOdate(1999,12,31,0,0,0,tz = "EST"), by = "10 min")
data$timeperiods = cut(data$timedate, breaks = tt)

data$periods <- sapply(strsplit(as.character(data$timeperiods) , " "), "[" , 2)

data$timeperiods <- as.character(data$timeperiods)
data$count <- as.numeric(ave(data$timeperiods, data$timeperiods, FUN = length))

data2 = select(data,date,periods,count)

data_wide <- reshape(data2, 
  timevar = "periods",
  idvar = c("date"),
  direction = "wide")

data_wide[is.na(data_wide)] = 0

#### reordering column by time
data_wide = data_wide[ , order(names(data_wide))]
```
After all the preprocessing the data in wide format look something like this:
```{r dataclean2, warning=FALSE}
head(data_wide, n = 3)
data_wide$date2 = strptime(data_wide$date,format='%y%m%d')
data_wide$days = weekdays(data_wide$date2)
data_wide$holidays = 1
for(i in 1:length(data_wide$date2)){
    for(count in 1:length(holidays)){
        if(sum(data_wide$date2[i]==holidays[count])>0){
            data_wide$holidays[i]=2}
    }
}
data_wide$Colour = ifelse(data_wide$days=="Friday",1,ifelse(data_wide$days=="Monday",2,ifelse(data_wide$days=="Saturday",3,ifelse(data_wide$days=="Sunday",4,ifelse(data_wide$days=="Thursday",5,ifelse(data_wide$days=="Tuesday",6,7))))))
#### deleting all times before 7AM when the center opens and date column and convert data to matrix format
X = as.matrix(data_wide[,-c(1:29,132:137)])
Xsvd = svd(X)
data_wide$date2 <- as.POSIXct(data_wide$date2)
data_fri = filter(data_wide,days=="Friday")
dim(data_fri)
data_sat = filter(data_wide,days=="Saturday")
data_rest = filter(data_wide, days!="Saturday" & days!="Friday")
X = as.matrix(data_wide[,-c(1:29,132:137)])
X_fri = as.matrix(data_fri[,-c(1:29,132:137)])
X_sat = as.matrix(data_sat[,-c(1:29,132:137)])
X_rest = as.matrix(data_rest[,-c(1:29,132:137)])
#### for poisson data apply transformation sqrt(x+1/4) to data to make more normal
Y = sqrt(X+1/4)
Y_fri = sqrt(X_fri+1/4)
Y_sat = sqrt(X_sat+1/4)
Y_rest = sqrt(X_rest+1/4)
```

## Scree Plots
The scree plot indicates that about 25% is explained by the 1st eigenvector, about 5% by the 2nd one and so on.

```{r scree, echo=FALSE}
barplot(Xsvd$d/sum(Xsvd$d), main = 'Scaled Eigenvalues for Call Data')
```

## Bi-Plots
The bi-plot indicates that the weekends (Fridays and Saturdays in Israel) are separate from the rest of the data. This indicates that for the prediction phase, it might be best to work on all of these separately. The triangles indicate +/-1 day from Holidays. Almost all the outlier seem to be either the weekends or Holidays.

```{r biplot, echo=FALSE, fig.width=6, fig.height=6}
plot(Xsvd$u[,1],Xsvd$u[,2],pch = data_wide$holidays, col=data_wide$Colour, ylab="First Eigenvector", xlab = "Second Eigenvector", main = "Biplot for Call Data")
legend("bottomleft",pch=1, col = 1:7, c("Friday","Monday","Saturday","Sunday","Thursday","Tuesday","Wednesday"),bty = "o")
```

## Model Evaluation
To evaulate the models we use Absolute Error(AE) where 

$$
AE_t = \frac{1}{T}\sum_{i = T+1}^{364} |\hat{y}_{it} - y_{it}|
$$

and $T$ is the size of the training dataset.

## Full dataset
Initially we do the evaluations on the full dataset (including weekends). We estimate $\mu_2$ by using the sample mean and estimate $\Sigma$ using the sample covariance method and CSCS. For CSCS, we need to pick a penalty parameter, which we do using cross-validation of the likelihood. 

```{r cv, warning=FALSE}
cv = function(lambda,K,train){
    cvec = rep(0,K)
    set.seed(12345)
    index = sample(1:dim(train)[1],replace=FALSE)
    foldsize = ceiling(dim(train)[1]/K)
    inTrain <- list()
    for(i in 1:(K-1)){
       inTrain[[i]] <- index[((i-1)*foldsize + 1): ((i-1)*foldsize + foldsize)]
   }
    inTrain[[K]] <- index[((K-1)*foldsize + 1):length(index)]
    for(k in 1:K){
        Ytrain = train[-c(inTrain[[k]]),]
        Ytest = train[c(inTrain[[k]]),]
        E = Ytrain
        out = CSCS2(as.matrix(E), lambda)$L
        Sigma_v = t(out)%*%out
        s_v = dim(Ytest)[1]
        sumterm = sum(diag(Ytest%*%(Sigma_v)%*%t(Ytest)))
        cvec[k] = s_v*log(det(solve(Sigma_v)))+sumterm
    }
    cv = (1/K)*sum(cvec)
    return(cv)
}
```

```{r full, warning=FALSE}
training = 1:252
train = scale(Y[training,], center = TRUE, scale = FALSE)
test = Y[-training,]
mu = colMeans(train)
S = (t(train)%*%train)/(dim(train)[1])
SError <- matrix(0,nrow=nrow(test),ncol=51)
for(k in 1:nrow(test)){
    S.mu2 <- predict.mean(test[k,1:51],mu=mu,Sigma=S)
    SError[k,] <- (S.mu2-as.numeric(test[k,52:102]))
}
E.S <- colMeans(abs(SError))
Time = 52:102
AE = E.S
Method = rep("S",length(AE))
Sdata = data.frame(Method,AE,Time)

lambdal = 0.01
lambda = seq(0.1,0.2,by=lambdal)
out = rep(0,length(lambda))
for(k in 1:length(lambda)){
    out[k] = cv(lambda[k], 5, train)
}
```

```{r plot, echo=FALSE}
plot(lambda, out, type = 'l', ylab = "CV", xlab = expression(lambda), main = "CV for CSCS")
abline(v = lambda[which.min(out)], col = 3)
```

```{r fullcont, warning=FALSE}
lambdastar = lambda[which.min(out)]
E = train
out = CSCS2(as.matrix(E), lambdastar)$L
CSCS= solve(t(out)%*%out)
CSCSError <- matrix(0,nrow=nrow(test),ncol=51)
for(k in 1:nrow(test)){
    CSCS.mu2 <- predict.mean(test[k,1:51],mu=mu,Sigma=CSCS)
    CSCSError[k,] <- (CSCS.mu2-as.numeric(test[k,52:102]))
}
E.CSCS <- colMeans(abs(CSCSError))
Time = 52:102
AE = E.CSCS
Method = rep("CSCS",length(AE))
CSCSdata = data.frame(Method,AE,Time)
```

The following plot shows the comparison of using the sample covariance matrix and CSCS on the full dataset. It's quite clear that CSCS does better than the sample covariance matrix in most cases.  

```{r comp1, echo=FALSE}
mergeddata = rbind(Sdata,CSCSdata)
pic<- ggplot(mergeddata, aes(x = Time, y=AE, group=Method, color=Method)) + 
    geom_line() + ggtitle("AE for all days")
### +theme(legend.position="none")
print(pic)
```

## Weekdays

This is the comparison for just the weekdays. Here, there isn't a clear winner. This is probably our dataset is more homogenous. CSCS is a more robust covariance estimator than the sample covaraince matrix. As we focussed on dataset with fewer outliers, the sample covariance matrix performed better comparitively.

```{r weekday, echo = FALSE, warning=FALSE}
training= 1:180
train = scale(Y_rest[training,], center = TRUE, scale = FALSE)
test = Y_rest[-training,]
mu = colMeans(train)
S = (t(train)%*%train)/(dim(train)[1])
SError <- matrix(0,nrow=nrow(test),ncol=51)
for(k in 1:nrow(test)){
    S.mu2 <- predict.mean(test[k,1:51],mu=mu,Sigma=S)
    SError[k,] <- (S.mu2-as.numeric(test[k,52:102]))
}
E.S <- colMeans(abs(SError))
Time = 52:102
AE = E.S
Method = rep("S",length(AE))
Sdata = data.frame(Method,AE,Time)

lambdal = 0.01
lambda = seq(0.1,0.2,by=lambdal)
out = rep(0,length(lambda))
for(k in 1:length(lambda)){
    out[k] = cv(lambda[k], 5, train)
}
```

```{r, echo=FALSE}
plot(lambda, out, type = 'l', ylab = "CV", xlab = expression(lambda), main = "CV for CSCS")
abline(v = lambda[which.min(out)], col = 3)
```

```{r, weekdaycont, echo=FALSE}
lambdastar = lambda[which.min(out)]
E = train
out = CSCS2(as.matrix(E), lambdastar)$L
CSCS= solve(t(out)%*%out)
CSCSError <- matrix(0,nrow=nrow(test),ncol=51)
for(k in 1:nrow(test)){
    CSCS.mu2 <- predict.mean(test[k,1:51],mu=mu,Sigma=CSCS)
    CSCSError[k,] <- (CSCS.mu2-as.numeric(test[k,52:102]))
}
E.CSCS <- colMeans(abs(CSCSError))
Time = 52:102
AE = E.CSCS
Method = rep("CSCS",length(AE))
CSCSdata = data.frame(Method,AE,Time)

mergeddata = rbind(Sdata,CSCSdata)
pic<- ggplot(mergeddata, aes(x = Time, y=AE, group=Method, color=Method)) + 
    geom_line() + ggtitle("AE for weekdays")
### +theme(legend.position="none")
print(pic)
```

```{r sums, echo=FALSE}
```
